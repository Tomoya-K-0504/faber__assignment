# faber__assignment
faber株式会社のインターンのための、課題の実装をあげています。

## 大まかなファイルと機能の説明
※パスをちゃんと記述して、cloneして実行できるようになっているものは文書分類のコードのみです。時間が足りませんでした。
 ファイルサイズの関係でlivedoor.jsonはアップロードできませんでしたが、辞書は作成済みなのでclassifier.pyは動きます。

### 文書分類
1. データの取得
  `make_json_data/py.py`にコードを書いています。
  livedoorコーパスを配布しているサイトにアクセスしてzipをダウンロードし、jsonデータを/data/rawに保存します。
  記事を一つひとつ読み込み、本文だけを残して/data/processedに保存しています。
  
2. モデルの構築と分類
  `classify.py`にコードを書きました。
  データを分かち書きして辞書を作成し(辞書は`/dic`に保存しています)、one-hot表現で文書をベクトル化して訓練データとします。
  モデルの構築と学習を行い、テストデータの分類の正答率を表示しています。
  また、各ジャンルの記事数、間違って分類したデータも出力しています。

### doc2vecを用いたレコメンド
1. データの取得
  `/scraper`の、TwitterScraper.pyで、あるユーザーのツイートを取得します。
  TwitterScraper2.pyで、あるキーワードをつぶやいている人を50名、そしてそれぞれのツイートを100件程度取得します。
  そのツイートは`/data/tweets`に、アカウントIDの名前で保存されています。
  
2. ユーザー辞書の構築と類似度計算
  `/word2vec.ipynb`に保存しています。時間がなく、.pyファイルにできておりません。
  gensimを使って、doc2vecでモデルを構築し、あるユーザーのツイートを学習します。
  次に、キーワードを元に取ってきたツイートを、ユーザーそれぞれの一つの文書として、一つ一つ類似度を計算します。(類似度の計算時に出たエラーが解消できませんでした)
  最後に、最も類似度が高いユーザーを、あるユーザーにレコメンドします。
  
以上で、課題とソースコードの説明を終わります。
